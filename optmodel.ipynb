{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EnvNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(14, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 13),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ObservationRAM(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # Observation space muss am Anfang festgelegt werden\n",
    "        self.observation_space = spaces.Box(low=0, high=210, shape=(13, ))\n",
    "\n",
    "    def observation(self, obs):\n",
    "        # Die Daten die benutzt werden (y, score, cooldown, auto x pos)\n",
    "        obs = obs[[14, 103, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117]].astype(float)\n",
    "\n",
    "        # normalisieren\n",
    "        obs[0] /= 176\n",
    "        obs[2] /= 142\n",
    "        for i in range(3, 13):\n",
    "            obs[i] /= 160\n",
    "\n",
    "        return obs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_action_sample():\n",
    "    # Random Actions wobei nach oben laufen am Wahrscheinlichsten ist\n",
    "    x = random.randint(0, 101)\n",
    "    if x < 90:\n",
    "        return 1\n",
    "    if x < 97:\n",
    "        return 2\n",
    "    return 0\n",
    "\n",
    "def get_env_data(n):\n",
    "    env_data = ObservationRAM(gym.make(\"ALE/Freeway-v5\", obs_type=\"ram\", render_mode=\"rgb_array\", difficulty=1, mode=3))\n",
    "\n",
    "    observation = env_data.reset()\n",
    "    df = pd.DataFrame(observation).T\n",
    "    # Actions: 0: nichts, 1: up, 2: down\n",
    "    actions = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # model bestimmt action\n",
    "        action = get_action_sample()\n",
    "\n",
    "        # observation 0 wird vor der schleife gespeichert\n",
    "        if i != 0:\n",
    "            df.loc[len(df)] = observation\n",
    "        actions.append(action)\n",
    "        observation, reward, done, info = env_data.step(action)\n",
    "        if done:\n",
    "            observation = env_data.reset()\n",
    "    env_data.close()\n",
    "\n",
    "    # df wird mit den richtigen Daten fertiggestellt\n",
    "    df[\"actions\"] = actions\n",
    "    df = df.rename(columns={0: \"y\", 1: \"score\", 2: \"cooldown\",\n",
    "                            3: \"car1\", 4: \"car2\", 5: \"car3\", 6: \"car4\", 7: \"car5\", 8: \"car6\", 9: \"car7\", 10: \"car8\", 11: \"car9\", 12: \"car10\"})\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_dfs(df):\n",
    "    df = df.tail(len(df) - 1)\n",
    "\n",
    "    dfY = df.copy()\n",
    "    dfY.drop([\"actions\"], axis=1, inplace=True)\n",
    "\n",
    "    dfY = dfY.drop(dfY.index[[0]])\n",
    "    df = df.drop(df.index[[len(df) - 1]])\n",
    "    dfY.index = df.index\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    dfY = dfY.reset_index(drop=True)\n",
    "    return df, dfY"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def train(X, y, model, loss_fn, optimizer, batch_size):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "\n",
    "    for i in range(round((len(y) / batch_size) + 0)):\n",
    "        # In jedem durchlauf wird der ganze batch auf dem gpu gespeichert\n",
    "        train_X = torch.from_numpy(X[i * batch_size:(i + 1) * batch_size]).cuda()\n",
    "        train_y = torch.from_numpy(y[i * batch_size:(i + 1) * batch_size]).cuda()\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        # loss wird für jeden Datensatz berechnet\n",
    "        for k in range(min(batch_size, len(train_X))):\n",
    "            pred = model.forward(train_X[k].float())\n",
    "            loss += loss_fn(pred.to(torch.float32), train_y[k].to(torch.float32))\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        # bp\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # return: avg loss\n",
    "    return loss_sum / len(y)\n",
    "\n",
    "\n",
    "def test(X, y, model, loss_fn, batch_size):\n",
    "    loss_sum = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(round((len(y) / batch_size) + 1)):\n",
    "            # In jedem durchlauf wird der ganze batch auf dem gpu gespeichert\n",
    "            test_X = torch.from_numpy(X[i * batch_size:(i + 1) * batch_size]).cuda()\n",
    "            test_y = torch.from_numpy(y[i * batch_size:(i + 1) * batch_size]).cuda()\n",
    "\n",
    "            # loss wird für jeden Datensatz berechnet\n",
    "            for k in range(min(batch_size, len(test_X))):\n",
    "                pred = model.forward(test_X[k].float())\n",
    "                loss = loss_fn(pred, test_y[k])\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "    loss_sum /= len(y)\n",
    "    print(f\"Avg loss: {loss_sum}!\")\n",
    "    return loss_sum\n",
    "\n",
    "def train_test_model(X, y, batch_size, learning_rate):\n",
    "    # X: input, y: target\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    # Reset parameters\n",
    "    model = EnvNetwork().cuda()\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, \"reset_parameters\"):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    last_test_avg = 10000\n",
    "    overfit = 0\n",
    "    epochs = 100\n",
    "\n",
    "    # Alle Dataframes zu numpy arrays\n",
    "    train_X = train_X.to_numpy()\n",
    "    test_X = test_X.to_numpy()\n",
    "    train_y = train_y.to_numpy()\n",
    "    test_y = test_y.to_numpy()\n",
    "\n",
    "    for t in range(epochs):\n",
    "        train_l = train(train_X, train_y, model, loss_fn, optimizer, batch_size)\n",
    "        test_avg = test(test_X, test_y, model, loss_fn, batch_size)\n",
    "\n",
    "        # Wenn der Test avg schlechter als vorher ist, wird der last_test_avg nicht aktualisiert\n",
    "        # und der overfit counter erhöht. Falls der counter >= 10 wird das training frühzeitig abgebrochen\n",
    "        if test_avg > last_test_avg:\n",
    "            overfit += 1\n",
    "        else:\n",
    "            overfit = 0\n",
    "            last_test_avg = test_avg\n",
    "        if overfit >= 10:\n",
    "            break\n",
    "    return test_avg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.00001, 0.01),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 16, 512)\n",
    "    }\n",
    "\n",
    "    loss = train_test_model(df, y, **params)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 22:23:33,838]\u001B[0m A new study created in memory with name: no-name-f9324ba0-fc87-497b-9fae-ee5f88790f9f\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.5636626430228825!\n",
      "Avg loss: 0.0969713022196376!\n",
      "Avg loss: 0.021625377405039056!\n",
      "Avg loss: 0.015294704822394397!\n",
      "Avg loss: 0.01314555542020537!\n",
      "Avg loss: 0.011575227847336814!\n",
      "Avg loss: 0.01027466408919354!\n",
      "Avg loss: 0.008936749808431213!\n",
      "Avg loss: 0.007759815481898551!\n",
      "Avg loss: 0.006740521613189311!\n",
      "Avg loss: 0.00529323974862979!\n",
      "Avg loss: 0.004657006646682839!\n",
      "Avg loss: 0.004267324886482623!\n",
      "Avg loss: 0.0038495241014821335!\n",
      "Avg loss: 0.0036769084704410273!\n",
      "Avg loss: 0.0036057749783825364!\n",
      "Avg loss: 0.0036189915817867223!\n",
      "Avg loss: 0.003792641306171805!\n",
      "Avg loss: 0.004181113681038873!\n",
      "Avg loss: 0.0034818213999590598!\n",
      "Avg loss: 0.0034012065062351327!\n",
      "Avg loss: 0.0033753256154562513!\n",
      "Avg loss: 0.0033987016030649175!\n",
      "Avg loss: 0.003437273391015276!\n",
      "Avg loss: 0.0034470888537070543!\n",
      "Avg loss: 0.003462563784621314!\n",
      "Avg loss: 0.003431373536860134!\n",
      "Avg loss: 0.0034380102302457314!\n",
      "Avg loss: 0.0036357670997363093!\n",
      "Avg loss: 0.0034887336991336424!\n",
      "Avg loss: 0.0033691557461674763!\n",
      "Avg loss: 0.0032546834500096215!\n",
      "Avg loss: 0.00316688810029886!\n",
      "Avg loss: 0.003145830135897771!\n",
      "Avg loss: 0.0032045328582066077!\n",
      "Avg loss: 0.0031819504712223942!\n",
      "Avg loss: 0.003145678114267634!\n",
      "Avg loss: 0.00315048697475487!\n",
      "Avg loss: 0.003161458247490289!\n",
      "Avg loss: 0.003128641329953408!\n",
      "Avg loss: 0.0030914895252034153!\n",
      "Avg loss: 0.003130459364325979!\n",
      "Avg loss: 0.0031534640732135298!\n",
      "Avg loss: 0.0031249332332638348!\n",
      "Avg loss: 0.00314142973519716!\n",
      "Avg loss: 0.003104185141527293!\n",
      "Avg loss: 0.0031274633375260714!\n",
      "Avg loss: 0.0030909875711575127!\n",
      "Avg loss: 0.003101684835240148!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 22:27:41,131]\u001B[0m Trial 0 finished with value: 0.003144685754638996 and parameters: {'learning_rate': 0.006385014523276346, 'batch_size': 267}. Best is trial 0 with value: 0.003144685754638996.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.003144685754638996!\n",
      "Avg loss: 0.597913809093455!\n",
      "Avg loss: 0.5664134896948179!\n",
      "Avg loss: 0.5624370819733284!\n",
      "Avg loss: 0.5604665843000154!\n",
      "Avg loss: 0.5580125170914483!\n",
      "Avg loss: 0.5544011059049136!\n",
      "Avg loss: 0.5533082914707901!\n",
      "Avg loss: 0.5528536297700504!\n",
      "Avg loss: 0.552585532239705!\n",
      "Avg loss: 0.5523425529304323!\n",
      "Avg loss: 0.5522337134453801!\n",
      "Avg loss: 0.5521860463133296!\n",
      "Avg loss: 0.5521336712307862!\n",
      "Avg loss: 0.5521127803590584!\n",
      "Avg loss: 0.5521045620046835!\n",
      "Avg loss: 0.552058264077832!\n",
      "Avg loss: 0.5520422456778344!\n",
      "Avg loss: 0.5520417833631891!\n",
      "Avg loss: 0.5520388588315919!\n",
      "Avg loss: 0.5520176293151682!\n",
      "Avg loss: 0.5520103206165918!\n",
      "Avg loss: 0.5520591595596033!\n",
      "Avg loss: 0.5520110737560286!\n",
      "Avg loss: 0.5519999364764501!\n",
      "Avg loss: 0.5520436121069713!\n",
      "Avg loss: 0.5519616707509762!\n",
      "Avg loss: 0.5519532756845519!\n",
      "Avg loss: 0.5519475834968759!\n",
      "Avg loss: 0.5519468312588999!\n",
      "Avg loss: 0.5519109052971172!\n",
      "Avg loss: 0.5519263670910273!\n",
      "Avg loss: 0.5519260104873632!\n",
      "Avg loss: 0.5519296613392444!\n",
      "Avg loss: 0.5518588883452561!\n",
      "Avg loss: 0.5518498649743168!\n",
      "Avg loss: 0.5517862170251447!\n",
      "Avg loss: 0.5518359268731912!\n",
      "Avg loss: 0.5518062310530903!\n",
      "Avg loss: 0.5517962725251009!\n",
      "Avg loss: 0.5518471889861847!\n",
      "Avg loss: 0.5518515094962846!\n",
      "Avg loss: 0.5518615552228434!\n",
      "Avg loss: 0.5518854867237787!\n",
      "Avg loss: 0.551843434773374!\n",
      "Avg loss: 0.5518230369759771!\n",
      "Avg loss: 0.5517578507702187!\n",
      "Avg loss: 0.5518465326257235!\n",
      "Avg loss: 0.5517624206741696!\n",
      "Avg loss: 0.5517931791411902!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 22:31:55,302]\u001B[0m Trial 1 finished with value: 0.5518691974468577 and parameters: {'learning_rate': 0.007758108994574983, 'batch_size': 157}. Best is trial 0 with value: 0.003144685754638996.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.5518691974468577!\n",
      "Avg loss: 0.03724319764294638!\n",
      "Avg loss: 0.03466982874890193!\n",
      "Avg loss: 0.032941396227044374!\n",
      "Avg loss: 0.03091537694425794!\n",
      "Avg loss: 0.029959994986949304!\n",
      "Avg loss: 0.029530683531062345!\n",
      "Avg loss: 0.029289779011676807!\n",
      "Avg loss: 0.029129367680422714!\n",
      "Avg loss: 0.029057849432983077!\n",
      "Avg loss: 0.028969691120553892!\n",
      "Avg loss: 0.028926047906139944!\n",
      "Avg loss: 0.028892441798416785!\n",
      "Avg loss: 0.028890409086291823!\n",
      "Avg loss: 0.028884954710126642!\n",
      "Avg loss: 0.028810634365823654!\n",
      "Avg loss: 0.028782219056673056!\n",
      "Avg loss: 0.002736425706594498!\n",
      "Avg loss: 0.002700836105462464!\n",
      "Avg loss: 0.002609925170590326!\n",
      "Avg loss: 0.0026401524533382317!\n",
      "Avg loss: 0.0025689491829083643!\n",
      "Avg loss: 0.002516193661930959!\n",
      "Avg loss: 0.0025436978068881766!\n",
      "Avg loss: 0.00252297656458196!\n",
      "Avg loss: 0.002465382117240834!\n",
      "Avg loss: 0.002458443180013214!\n",
      "Avg loss: 0.002501624986005703!\n",
      "Avg loss: 0.00248561530333023!\n",
      "Avg loss: 0.002496601354252271!\n",
      "Avg loss: 0.0024572145961760795!\n",
      "Avg loss: 0.0025092807001086954!\n",
      "Avg loss: 0.002464699021564295!\n",
      "Avg loss: 0.002456274904850294!\n",
      "Avg loss: 0.0024336742130891416!\n",
      "Avg loss: 0.0024172221589049948!\n",
      "Avg loss: 0.0023746969563329576!\n",
      "Avg loss: 0.002379285592353343!\n",
      "Avg loss: 0.0024033025404900364!\n",
      "Avg loss: 0.002401749538485342!\n",
      "Avg loss: 0.0023575737788608404!\n",
      "Avg loss: 0.002339370020237006!\n",
      "Avg loss: 0.002313242800424587!\n",
      "Avg loss: 0.0023261073563091263!\n",
      "Avg loss: 0.002303017958513602!\n",
      "Avg loss: 0.002343353662709109!\n",
      "Avg loss: 0.002289594783745453!\n",
      "Avg loss: 0.002258812501349802!\n",
      "Avg loss: 0.002301731704951703!\n",
      "Avg loss: 0.0022791988092286087!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 22:36:37,675]\u001B[0m Trial 2 finished with value: 0.002254438832056477 and parameters: {'learning_rate': 0.0012333730272013918, 'batch_size': 16}. Best is trial 2 with value: 0.002254438832056477.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.002254438832056477!\n",
      "Avg loss: 0.10070889472846346!\n",
      "Avg loss: 0.06339670025124142!\n",
      "Avg loss: 0.04834571486863889!\n",
      "Avg loss: 0.04162927121527472!\n",
      "Avg loss: 0.0386201432839924!\n",
      "Avg loss: 0.03684062939614839!\n",
      "Avg loss: 0.03601559762260846!\n",
      "Avg loss: 0.03556690175668688!\n",
      "Avg loss: 0.03513409010016662!\n",
      "Avg loss: 0.03464862284466686!\n",
      "Avg loss: 0.03424078861072636!\n",
      "Avg loss: 0.03359684588397425!\n",
      "Avg loss: 0.033264666009264456!\n",
      "Avg loss: 0.0328904538057654!\n",
      "Avg loss: 0.032488346048393235!\n",
      "Avg loss: 0.03172349576855159!\n",
      "Avg loss: 0.030821469648065716!\n",
      "Avg loss: 0.03017296514243147!\n",
      "Avg loss: 0.029728838492832074!\n",
      "Avg loss: 0.029272012925719224!\n",
      "Avg loss: 0.029017702103363657!\n",
      "Avg loss: 0.02868343183304928!\n",
      "Avg loss: 0.028425014880298794!\n",
      "Avg loss: 0.02826289505491171!\n",
      "Avg loss: 0.028086046429904633!\n",
      "Avg loss: 0.027904157244572875!\n",
      "Avg loss: 0.02777653677667276!\n",
      "Avg loss: 0.02770037713469154!\n",
      "Avg loss: 0.027637729479402597!\n",
      "Avg loss: 0.027587819232070997!\n",
      "Avg loss: 0.027541726893490118!\n",
      "Avg loss: 0.02750685653596879!\n",
      "Avg loss: 0.027472270837323034!\n",
      "Avg loss: 0.0274408259483654!\n",
      "Avg loss: 0.027401502319008366!\n",
      "Avg loss: 0.02736585973861692!\n",
      "Avg loss: 0.027334611307213576!\n",
      "Avg loss: 0.027309783017921647!\n",
      "Avg loss: 0.027287223671784485!\n",
      "Avg loss: 0.027267119583153553!\n",
      "Avg loss: 0.027246448122008233!\n",
      "Avg loss: 0.027229141314775204!\n",
      "Avg loss: 0.027214788610986973!\n",
      "Avg loss: 0.027201698961906804!\n",
      "Avg loss: 0.0271898232971163!\n",
      "Avg loss: 0.027180249978655164!\n",
      "Avg loss: 0.027174034070431254!\n",
      "Avg loss: 0.02716402275702379!\n",
      "Avg loss: 0.027158015723554828!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 22:40:46,351]\u001B[0m Trial 3 finished with value: 0.02715536305858739 and parameters: {'learning_rate': 0.002356019225086773, 'batch_size': 433}. Best is trial 2 with value: 0.002254438832056477.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.02715536305858739!\n",
      "Avg loss: 0.06892879847650393!\n",
      "Avg loss: 0.03702676212284985!\n",
      "Avg loss: 0.025848541785806766!\n",
      "Avg loss: 0.02137360271013181!\n",
      "Avg loss: 0.018882430945427332!\n",
      "Avg loss: 0.017722826090853593!\n",
      "Avg loss: 0.01681159956847624!\n",
      "Avg loss: 0.016123652309145323!\n",
      "Avg loss: 0.015380160214326071!\n",
      "Avg loss: 0.014593774564285152!\n",
      "Avg loss: 0.013741835604931403!\n",
      "Avg loss: 0.012981753248605923!\n",
      "Avg loss: 0.01226776074753478!\n",
      "Avg loss: 0.011559069205078898!\n",
      "Avg loss: 0.010888754495211095!\n",
      "Avg loss: 0.010357031963296631!\n",
      "Avg loss: 0.010040847497582347!\n",
      "Avg loss: 0.009820900832053253!\n",
      "Avg loss: 0.00959213120296657!\n",
      "Avg loss: 0.009449870146743589!\n",
      "Avg loss: 0.009374366069122434!\n",
      "Avg loss: 0.009271478535394699!\n",
      "Avg loss: 0.009163187419371523!\n",
      "Avg loss: 0.00909740525159879!\n",
      "Avg loss: 0.00904133225983711!\n",
      "Avg loss: 0.009006081747764383!\n",
      "Avg loss: 0.008985104622350921!\n",
      "Avg loss: 0.008964504418567177!\n",
      "Avg loss: 0.008936505190319325!\n",
      "Avg loss: 0.0089084331212143!\n",
      "Avg loss: 0.008889698164908916!\n",
      "Avg loss: 0.008875120121706756!\n",
      "Avg loss: 0.008877758915896409!\n",
      "Avg loss: 0.008854692153121253!\n",
      "Avg loss: 0.008810692395038476!\n",
      "Avg loss: 0.008807561429802633!\n",
      "Avg loss: 0.008802421465658128!\n",
      "Avg loss: 0.00884605400381911!\n",
      "Avg loss: 0.008769816508449414!\n",
      "Avg loss: 0.00869341936782307!\n",
      "Avg loss: 0.00862940369148208!\n",
      "Avg loss: 0.008612206197982721!\n",
      "Avg loss: 0.008617905750868092!\n",
      "Avg loss: 0.008649170088996447!\n",
      "Avg loss: 0.008639188194382771!\n",
      "Avg loss: 0.008640076976186458!\n",
      "Avg loss: 0.008635903670123493!\n",
      "Avg loss: 0.00859614234234562!\n",
      "Avg loss: 0.008573338841865171!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 22:44:52,280]\u001B[0m Trial 4 finished with value: 0.008667451196618867 and parameters: {'learning_rate': 0.005410207592492016, 'batch_size': 463}. Best is trial 2 with value: 0.002254438832056477.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.008667451196618867!\n",
      "Avg loss: 0.5790169046569397!\n",
      "Avg loss: 0.5579350745769599!\n",
      "Avg loss: 0.5481186125583576!\n",
      "Avg loss: 0.5453065129128413!\n",
      "Avg loss: 0.5438103100278613!\n",
      "Avg loss: 0.5425923953949029!\n",
      "Avg loss: 0.5418673374083025!\n",
      "Avg loss: 0.5411462675835458!\n",
      "Avg loss: 0.5395360621349042!\n",
      "Avg loss: 0.5390477108176659!\n",
      "Avg loss: 0.5384273613343848!\n",
      "Avg loss: 0.5377620047337962!\n",
      "Avg loss: 0.537291695150893!\n",
      "Avg loss: 0.5371888197602621!\n",
      "Avg loss: 0.5371132737554284!\n",
      "Avg loss: 0.5370424021313832!\n",
      "Avg loss: 0.5370391299242517!\n",
      "Avg loss: 0.5370076829882213!\n",
      "Avg loss: 0.536947992012542!\n",
      "Avg loss: 0.5369185823552223!\n",
      "Avg loss: 0.5368959455955389!\n",
      "Avg loss: 0.5368726576445865!\n",
      "Avg loss: 0.5368372061189384!\n",
      "Avg loss: 0.5368292019308896!\n",
      "Avg loss: 0.5368424368285044!\n",
      "Avg loss: 0.536841648515925!\n",
      "Avg loss: 0.5368084202914764!\n",
      "Avg loss: 0.5367965597133857!\n",
      "Avg loss: 0.5367776058283419!\n",
      "Avg loss: 0.5367593379936436!\n",
      "Avg loss: 0.5367198872043039!\n",
      "Avg loss: 0.5367221539534822!\n",
      "Avg loss: 0.5367396984406027!\n",
      "Avg loss: 0.5367135426544096!\n",
      "Avg loss: 0.5367288470703084!\n",
      "Avg loss: 0.5366917772930239!\n",
      "Avg loss: 0.5367110166504225!\n",
      "Avg loss: 0.5366793752454404!\n",
      "Avg loss: 0.5366992452555354!\n",
      "Avg loss: 0.5366708227862466!\n",
      "Avg loss: 0.5367034229230678!\n",
      "Avg loss: 0.5366491068388379!\n",
      "Avg loss: 0.5366588579893073!\n",
      "Avg loss: 0.536656049932394!\n",
      "Avg loss: 0.5366412188389124!\n",
      "Avg loss: 0.5366282577606247!\n",
      "Avg loss: 0.5366135422423398!\n",
      "Avg loss: 0.5366056674161829!\n",
      "Avg loss: 0.5365577848805437!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 22:49:03,271]\u001B[0m Trial 5 finished with value: 0.5365406587535962 and parameters: {'learning_rate': 0.007016589381381467, 'batch_size': 360}. Best is trial 2 with value: 0.002254438832056477.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.5365406587535962!\n",
      "Avg loss: 0.04489714696600319!\n",
      "Avg loss: 0.03829797757530328!\n",
      "Avg loss: 0.035842396931154284!\n",
      "Avg loss: 0.03372384504064028!\n",
      "Avg loss: 0.03202969735132733!\n",
      "Avg loss: 0.030148659581568465!\n",
      "Avg loss: 0.028945459196157892!\n",
      "Avg loss: 0.028449443681544995!\n",
      "Avg loss: 0.028239582001857843!\n",
      "Avg loss: 0.028034574055263762!\n",
      "Avg loss: 0.027936637246474993!\n",
      "Avg loss: 0.0279490310989246!\n",
      "Avg loss: 0.02795521550316704!\n",
      "Avg loss: 0.02789745595269352!\n",
      "Avg loss: 0.02778231334468248!\n",
      "Avg loss: 0.02781261503755938!\n",
      "Avg loss: 0.027803615460517055!\n",
      "Avg loss: 0.02780975072379678!\n",
      "Avg loss: 0.02780515497550252!\n",
      "Avg loss: 0.027833184434520316!\n",
      "Avg loss: 0.027856460100994247!\n",
      "Avg loss: 0.027846531803496898!\n",
      "Avg loss: 0.02780224834669541!\n",
      "Avg loss: 0.02780616818240382!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 22:51:09,440]\u001B[0m Trial 6 finished with value: 0.027789744481034898 and parameters: {'learning_rate': 0.004367076761768438, 'batch_size': 131}. Best is trial 2 with value: 0.002254438832056477.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.027789744481034898!\n",
      "Avg loss: 0.04945141513285378!\n",
      "Avg loss: 0.023172385134365724!\n",
      "Avg loss: 0.017248124857703818!\n",
      "Avg loss: 0.014505294903549291!\n",
      "Avg loss: 0.012818846538199376!\n",
      "Avg loss: 0.011498304919848008!\n",
      "Avg loss: 0.01055977938836907!\n",
      "Avg loss: 0.00945796751442209!\n",
      "Avg loss: 0.008450947532226102!\n",
      "Avg loss: 0.007610651736367157!\n",
      "Avg loss: 0.006441662329633431!\n",
      "Avg loss: 0.005437778101596638!\n",
      "Avg loss: 0.004641943903665082!\n",
      "Avg loss: 0.003998542197613964!\n",
      "Avg loss: 0.0036424721246333023!\n",
      "Avg loss: 0.003242606300934175!\n",
      "Avg loss: 0.0029710369482518826!\n",
      "Avg loss: 0.0028887970694508784!\n",
      "Avg loss: 0.0028152762041770136!\n",
      "Avg loss: 0.002721849435094736!\n",
      "Avg loss: 0.0025722894821605846!\n",
      "Avg loss: 0.002480544526302532!\n",
      "Avg loss: 0.002456810038675645!\n",
      "Avg loss: 0.0023430918996790286!\n",
      "Avg loss: 0.0023341656855158334!\n",
      "Avg loss: 0.0022890315102404046!\n",
      "Avg loss: 0.0022948728287715695!\n",
      "Avg loss: 0.0022907084963689948!\n",
      "Avg loss: 0.002289527862962482!\n",
      "Avg loss: 0.0023028372136828026!\n",
      "Avg loss: 0.0022948162179089603!\n",
      "Avg loss: 0.002306680346522635!\n",
      "Avg loss: 0.0023074464656045175!\n",
      "Avg loss: 0.002421753651843625!\n",
      "Avg loss: 0.0023456357143165287!\n",
      "Avg loss: 0.002261568500168242!\n",
      "Avg loss: 0.0022608221756695752!\n",
      "Avg loss: 0.0022559436077521293!\n",
      "Avg loss: 0.002263015280020018!\n",
      "Avg loss: 0.0022720890286094266!\n",
      "Avg loss: 0.0022630532615102424!\n",
      "Avg loss: 0.0022682612848604097!\n",
      "Avg loss: 0.0022584875420769645!\n",
      "Avg loss: 0.0022615389139953724!\n",
      "Avg loss: 0.0022498103023590107!\n",
      "Avg loss: 0.002277788779870794!\n",
      "Avg loss: 0.002254629391337087!\n",
      "Avg loss: 0.0022551156276851753!\n",
      "Avg loss: 0.0022484561728153617!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 22:55:21,827]\u001B[0m Trial 7 finished with value: 0.0022408253588251806 and parameters: {'learning_rate': 0.003864343378554892, 'batch_size': 276}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.0022408253588251806!\n",
      "Avg loss: 0.2842527007312232!\n",
      "Avg loss: 0.10755298771218211!\n",
      "Avg loss: 0.05519474146576738!\n",
      "Avg loss: 0.046089653283413606!\n",
      "Avg loss: 0.03815592297484795!\n",
      "Avg loss: 0.03259477179049741!\n",
      "Avg loss: 0.029547476167682885!\n",
      "Avg loss: 0.027689185649528533!\n",
      "Avg loss: 0.026143453508202522!\n",
      "Avg loss: 0.024939080901483716!\n",
      "Avg loss: 0.0240840811147573!\n",
      "Avg loss: 0.023467517183630115!\n",
      "Avg loss: 0.023011602814888815!\n",
      "Avg loss: 0.022614613174711737!\n",
      "Avg loss: 0.022276504317115316!\n",
      "Avg loss: 0.021973821099492776!\n",
      "Avg loss: 0.021686990030310667!\n",
      "Avg loss: 0.0214079812688865!\n",
      "Avg loss: 0.02113223208948837!\n",
      "Avg loss: 0.020864446987802487!\n",
      "Avg loss: 0.020589987143474594!\n",
      "Avg loss: 0.02030616990340025!\n",
      "Avg loss: 0.02003085411267395!\n",
      "Avg loss: 0.019759484812752047!\n",
      "Avg loss: 0.019487190080074625!\n",
      "Avg loss: 0.01921614068550906!\n",
      "Avg loss: 0.018948023755845363!\n",
      "Avg loss: 0.018683625011871312!\n",
      "Avg loss: 0.018413395659473326!\n",
      "Avg loss: 0.01816234769849501!\n",
      "Avg loss: 0.01791315875982659!\n",
      "Avg loss: 0.017673389964003076!\n",
      "Avg loss: 0.017435641792719087!\n",
      "Avg loss: 0.017172689380327052!\n",
      "Avg loss: 0.01691812291846847!\n",
      "Avg loss: 0.016660709644840187!\n",
      "Avg loss: 0.016437827941161925!\n",
      "Avg loss: 0.016224937930843775!\n",
      "Avg loss: 0.016026176827461002!\n",
      "Avg loss: 0.015832530826929675!\n",
      "Avg loss: 0.015653607334909052!\n",
      "Avg loss: 0.015486694592172952!\n",
      "Avg loss: 0.015330914744612371!\n",
      "Avg loss: 0.015183105287862753!\n",
      "Avg loss: 0.015048199359719431!\n",
      "Avg loss: 0.014919496789679049!\n",
      "Avg loss: 0.014803890250303984!\n",
      "Avg loss: 0.014698028855745946!\n",
      "Avg loss: 0.014599937979571886!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 22:59:34,548]\u001B[0m Trial 8 finished with value: 0.014511888982039494 and parameters: {'learning_rate': 0.000760660716175433, 'batch_size': 495}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.014511888982039494!\n",
      "Avg loss: 0.03608061358752859!\n",
      "Avg loss: 0.022946931410371046!\n",
      "Avg loss: 0.018588723265479452!\n",
      "Avg loss: 0.016132172183881962!\n",
      "Avg loss: 0.015033406323217547!\n",
      "Avg loss: 0.013420967798639907!\n",
      "Avg loss: 0.011334366301286503!\n",
      "Avg loss: 0.01058474983267989!\n",
      "Avg loss: 0.009479273828254057!\n",
      "Avg loss: 0.008972715501964377!\n",
      "Avg loss: 0.008743620595742019!\n",
      "Avg loss: 0.008735373366529632!\n",
      "Avg loss: 0.008679587904325088!\n",
      "Avg loss: 0.008619425513123105!\n",
      "Avg loss: 0.008503614838299074!\n",
      "Avg loss: 0.008474384029192894!\n",
      "Avg loss: 0.008442780397805619!\n",
      "Avg loss: 0.00841563651087139!\n",
      "Avg loss: 0.008405633156912097!\n",
      "Avg loss: 0.008398665716543867!\n",
      "Avg loss: 0.00832593646810868!\n",
      "Avg loss: 0.008273822967312786!\n",
      "Avg loss: 0.008185676318662245!\n",
      "Avg loss: 0.00813591511596092!\n",
      "Avg loss: 0.008127615755767224!\n",
      "Avg loss: 0.008055132637415382!\n",
      "Avg loss: 0.007992590007433971!\n",
      "Avg loss: 0.007931514495029102!\n",
      "Avg loss: 0.007924724218521372!\n",
      "Avg loss: 0.007943748142143894!\n",
      "Avg loss: 0.007929869933652424!\n",
      "Avg loss: 0.007976059587396021!\n",
      "Avg loss: 0.007855979802589366!\n",
      "Avg loss: 0.007988532812468606!\n",
      "Avg loss: 0.007939659148528538!\n",
      "Avg loss: 0.007931184562696963!\n",
      "Avg loss: 0.007921346724829413!\n",
      "Avg loss: 0.007877922646325499!\n",
      "Avg loss: 0.007847420630777872!\n",
      "Avg loss: 0.00781965878552999!\n",
      "Avg loss: 0.007773987544821723!\n",
      "Avg loss: 0.007770298371196936!\n",
      "Avg loss: 0.007754441518952611!\n",
      "Avg loss: 0.007734563189328274!\n",
      "Avg loss: 0.007720527124239007!\n",
      "Avg loss: 0.007739137589134305!\n",
      "Avg loss: 0.007741116781410466!\n",
      "Avg loss: 0.007741980549046066!\n",
      "Avg loss: 0.007718795719964453!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:03:48,815]\u001B[0m Trial 9 finished with value: 0.007737301504687432 and parameters: {'learning_rate': 0.005277414164201216, 'batch_size': 232}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.007737301504687432!\n",
      "Avg loss: 0.08391427846493645!\n",
      "Avg loss: 0.05777571391517768!\n",
      "Avg loss: 0.04490790916937156!\n",
      "Avg loss: 0.042628981107727316!\n",
      "Avg loss: 0.04154631063041651!\n",
      "Avg loss: 0.040298961298187864!\n",
      "Avg loss: 0.03919999283442281!\n",
      "Avg loss: 0.015949988840486755!\n",
      "Avg loss: 0.012832291983712354!\n",
      "Avg loss: 0.011067627511274785!\n",
      "Avg loss: 0.009492889504933603!\n",
      "Avg loss: 0.009325303232956926!\n",
      "Avg loss: 0.009256489406377874!\n",
      "Avg loss: 0.009033113621934166!\n",
      "Avg loss: 0.008880492747499785!\n",
      "Avg loss: 0.00868043060040005!\n",
      "Avg loss: 0.00851048563528027!\n",
      "Avg loss: 0.008501872330739674!\n",
      "Avg loss: 0.008485261831615474!\n",
      "Avg loss: 0.008474991164630716!\n",
      "Avg loss: 0.008502428718036385!\n",
      "Avg loss: 0.008580319674931298!\n",
      "Avg loss: 0.008564282327284773!\n",
      "Avg loss: 0.008619268362326639!\n",
      "Avg loss: 0.008575062479853504!\n",
      "Avg loss: 0.008577713349475762!\n",
      "Avg loss: 0.008501424238045736!\n",
      "Avg loss: 0.008533659664340242!\n",
      "Avg loss: 0.00855262896288863!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:06:07,407]\u001B[0m Trial 10 finished with value: 0.008587054732016399 and parameters: {'learning_rate': 0.009966417017568011, 'batch_size': 331}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.008587054732016399!\n",
      "Avg loss: 0.6013209360930547!\n",
      "Avg loss: 0.5960811549164882!\n",
      "Avg loss: 0.5934123342798387!\n",
      "Avg loss: 0.5902646579628675!\n",
      "Avg loss: 0.5883933254755859!\n",
      "Avg loss: 0.5629243255759965!\n",
      "Avg loss: 0.5615739739781075!\n",
      "Avg loss: 0.5611674599507063!\n",
      "Avg loss: 0.5610177799629051!\n",
      "Avg loss: 0.5609099863450904!\n",
      "Avg loss: 0.5608589241273004!\n",
      "Avg loss: 0.5608018224895759!\n",
      "Avg loss: 0.5607737818244055!\n",
      "Avg loss: 0.5607565275936838!\n",
      "Avg loss: 0.5607135640322853!\n",
      "Avg loss: 0.5607123348668209!\n",
      "Avg loss: 0.5607463142514558!\n",
      "Avg loss: 0.5607381860771662!\n",
      "Avg loss: 0.5607533809973902!\n",
      "Avg loss: 0.5607674385315448!\n",
      "Avg loss: 0.5606903746128529!\n",
      "Avg loss: 0.5607420199537178!\n",
      "Avg loss: 0.5607833724123654!\n",
      "Avg loss: 0.5607208374344786!\n",
      "Avg loss: 0.5607936399426544!\n",
      "Avg loss: 0.5607844860351747!\n",
      "Avg loss: 0.5607403029700978!\n",
      "Avg loss: 0.5608296565566898!\n",
      "Avg loss: 0.5607596450142242!\n",
      "Avg loss: 0.5607238342049257!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:08:27,802]\u001B[0m Trial 11 finished with value: 0.5607215302037387 and parameters: {'learning_rate': 0.002887640945208067, 'batch_size': 91}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.5607215302037387!\n",
      "Avg loss: 0.18938517438924687!\n",
      "Avg loss: 0.08197204280271451!\n",
      "Avg loss: 0.045801836288427274!\n",
      "Avg loss: 0.03564203402412086!\n",
      "Avg loss: 0.028996496525571548!\n",
      "Avg loss: 0.02395736513929099!\n",
      "Avg loss: 0.020746671952360275!\n",
      "Avg loss: 0.01882235971715853!\n",
      "Avg loss: 0.01753272451958366!\n",
      "Avg loss: 0.01647943142589467!\n",
      "Avg loss: 0.01554215843799272!\n",
      "Avg loss: 0.01468925505253057!\n",
      "Avg loss: 0.013948203604671302!\n",
      "Avg loss: 0.01329403397296932!\n",
      "Avg loss: 0.01274671672362567!\n",
      "Avg loss: 0.012277880914616136!\n",
      "Avg loss: 0.01189607312429172!\n",
      "Avg loss: 0.011592402811534154!\n",
      "Avg loss: 0.011337600944158706!\n",
      "Avg loss: 0.01112089503956945!\n",
      "Avg loss: 0.01092419800796248!\n",
      "Avg loss: 0.010738034660930918!\n",
      "Avg loss: 0.010560348106358877!\n",
      "Avg loss: 0.010392983934692769!\n",
      "Avg loss: 0.010228508416986795!\n",
      "Avg loss: 0.010061739703968894!\n",
      "Avg loss: 0.00989475468855989!\n",
      "Avg loss: 0.009720825880675714!\n",
      "Avg loss: 0.009554707057694876!\n",
      "Avg loss: 0.009385198119207436!\n",
      "Avg loss: 0.009216115872450383!\n",
      "Avg loss: 0.009046482716978263!\n",
      "Avg loss: 0.008886054260482754!\n",
      "Avg loss: 0.008725208550349566!\n",
      "Avg loss: 0.008569335785267368!\n",
      "Avg loss: 0.008418576281867691!\n",
      "Avg loss: 0.008275323538285806!\n",
      "Avg loss: 0.008131524127554576!\n",
      "Avg loss: 0.007989603659455316!\n",
      "Avg loss: 0.007853555725637121!\n",
      "Avg loss: 0.007716258190246695!\n",
      "Avg loss: 0.007581835514031655!\n",
      "Avg loss: 0.0074476375800101085!\n",
      "Avg loss: 0.007314643213146483!\n",
      "Avg loss: 0.007182380936540873!\n",
      "Avg loss: 0.007051026980214054!\n",
      "Avg loss: 0.006912031517104773!\n",
      "Avg loss: 0.006781482843535341!\n",
      "Avg loss: 0.006644848057044551!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:12:12,530]\u001B[0m Trial 12 finished with value: 0.006511468059631038 and parameters: {'learning_rate': 0.00023450349279280662, 'batch_size': 221}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.006511468059631038!\n",
      "Avg loss: 0.5787863407501962!\n",
      "Avg loss: 0.5766565379173758!\n",
      "Avg loss: 0.572983636623221!\n",
      "Avg loss: 0.5697315458011157!\n",
      "Avg loss: 0.5680428831814811!\n",
      "Avg loss: 0.5674718563687239!\n",
      "Avg loss: 0.5673127719091107!\n",
      "Avg loss: 0.56729788572354!\n",
      "Avg loss: 0.5672972532948959!\n",
      "Avg loss: 0.5672352895744956!\n",
      "Avg loss: 0.5671594091271083!\n",
      "Avg loss: 0.5672561597859859!\n",
      "Avg loss: 0.5671501031202635!\n",
      "Avg loss: 0.5671739230798792!\n",
      "Avg loss: 0.5670348901574237!\n",
      "Avg loss: 0.5671001178187802!\n",
      "Avg loss: 0.5671401546660663!\n",
      "Avg loss: 0.5671764344539305!\n",
      "Avg loss: 0.5670973589021856!\n",
      "Avg loss: 0.5671988503229362!\n",
      "Avg loss: 0.5670675791265708!\n",
      "Avg loss: 0.5670050587905724!\n",
      "Avg loss: 0.5670726363690309!\n",
      "Avg loss: 0.5668596764288495!\n",
      "Avg loss: 0.56702467214677!\n",
      "Avg loss: 0.5411536433040822!\n",
      "Avg loss: 0.5410284200151536!\n",
      "Avg loss: 0.5410549151591811!\n",
      "Avg loss: 0.5410889269831974!\n",
      "Avg loss: 0.5410347995332789!\n",
      "Avg loss: 0.5410202605231417!\n",
      "Avg loss: 0.5410608847464257!\n",
      "Avg loss: 0.5409515185688097!\n",
      "Avg loss: 0.5408958189756391!\n",
      "Avg loss: 0.5409463071221051!\n",
      "Avg loss: 0.5410363450379658!\n",
      "Avg loss: 0.5408962891485937!\n",
      "Avg loss: 0.5409787262631987!\n",
      "Avg loss: 0.5408293214159313!\n",
      "Avg loss: 0.5408505217493643!\n",
      "Avg loss: 0.5408011858941199!\n",
      "Avg loss: 0.540850551832226!\n",
      "Avg loss: 0.5410236620330411!\n",
      "Avg loss: 0.5407866132226234!\n",
      "Avg loss: 0.5408011999206462!\n",
      "Avg loss: 0.5406828667588993!\n",
      "Avg loss: 0.5407142273713702!\n",
      "Avg loss: 0.5407304126880013!\n",
      "Avg loss: 0.5407801042142478!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:16:08,404]\u001B[0m Trial 13 finished with value: 0.5408380876546597 and parameters: {'learning_rate': 0.0025825736687053523, 'batch_size': 35}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.5408380876546597!\n",
      "Avg loss: 0.03607122338102458!\n",
      "Avg loss: 0.010053149020129604!\n",
      "Avg loss: 0.007942811702157517!\n",
      "Avg loss: 0.005205561282225147!\n",
      "Avg loss: 0.004173722611641037!\n",
      "Avg loss: 0.003785924604800049!\n",
      "Avg loss: 0.0035649811776524883!\n",
      "Avg loss: 0.003381402377503183!\n",
      "Avg loss: 0.0033135441089559236!\n",
      "Avg loss: 0.0032410467174699385!\n",
      "Avg loss: 0.003175659922144867!\n",
      "Avg loss: 0.003157114581703902!\n",
      "Avg loss: 0.003176352027451694!\n",
      "Avg loss: 0.003206628181301929!\n",
      "Avg loss: 0.003223050387942305!\n",
      "Avg loss: 0.0031606904223577363!\n",
      "Avg loss: 0.0031594597010594174!\n",
      "Avg loss: 0.0032443422116543814!\n",
      "Avg loss: 0.0032130821864243334!\n",
      "Avg loss: 0.0031752911676105394!\n",
      "Avg loss: 0.0031084974767973932!\n",
      "Avg loss: 0.0029719708417571744!\n",
      "Avg loss: 0.0029362537773823315!\n",
      "Avg loss: 0.002961263884173743!\n",
      "Avg loss: 0.0029198785549928133!\n",
      "Avg loss: 0.002902509170878061!\n",
      "Avg loss: 0.002858417490943687!\n",
      "Avg loss: 0.0028609488491450424!\n",
      "Avg loss: 0.00280734386676219!\n",
      "Avg loss: 0.0029205810744772497!\n",
      "Avg loss: 0.0028533223629714404!\n",
      "Avg loss: 0.00281816084742712!\n",
      "Avg loss: 0.0028252487488514555!\n",
      "Avg loss: 0.0028841651988947116!\n",
      "Avg loss: 0.002905305099911527!\n",
      "Avg loss: 0.002845242421676376!\n",
      "Avg loss: 0.0027888793152685376!\n",
      "Avg loss: 0.0028218661741546656!\n",
      "Avg loss: 0.0028295049373885635!\n",
      "Avg loss: 0.0028499990915663107!\n",
      "Avg loss: 0.002807532856346761!\n",
      "Avg loss: 0.0027988213374691533!\n",
      "Avg loss: 0.0027964990989797836!\n",
      "Avg loss: 0.00276616678909106!\n",
      "Avg loss: 0.002831616080558168!\n",
      "Avg loss: 0.002810219360340606!\n",
      "Avg loss: 0.0027961286324561588!\n",
      "Avg loss: 0.0027601228951662747!\n",
      "Avg loss: 0.002767648949095316!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:20:13,161]\u001B[0m Trial 14 finished with value: 0.002753613201512292 and parameters: {'learning_rate': 0.0015197017485872079, 'batch_size': 20}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.002753613201512292!\n",
      "Avg loss: 0.10636240155636431!\n",
      "Avg loss: 0.07629252924789136!\n",
      "Avg loss: 0.06838892141904158!\n",
      "Avg loss: 0.06621682614993879!\n",
      "Avg loss: 0.046620169295020344!\n",
      "Avg loss: 0.03912947015908037!\n",
      "Avg loss: 0.03770795648265295!\n",
      "Avg loss: 0.03636876728207827!\n",
      "Avg loss: 0.03498736234451444!\n",
      "Avg loss: 0.03368210924925216!\n",
      "Avg loss: 0.025232291257331854!\n",
      "Avg loss: 0.008195464564038493!\n",
      "Avg loss: 0.005118390012222805!\n",
      "Avg loss: 0.004441424829436915!\n",
      "Avg loss: 0.004148727466640564!\n",
      "Avg loss: 0.003931445657328051!\n",
      "Avg loss: 0.0037510108334194805!\n",
      "Avg loss: 0.0036240224187684554!\n",
      "Avg loss: 0.003552226277070731!\n",
      "Avg loss: 0.003503627712250791!\n",
      "Avg loss: 0.003480993861217515!\n",
      "Avg loss: 0.0034584931358979895!\n",
      "Avg loss: 0.0034171451307334626!\n",
      "Avg loss: 0.003366400590305999!\n",
      "Avg loss: 0.0033238764756625032!\n",
      "Avg loss: 0.00329320312020358!\n",
      "Avg loss: 0.003259222450826998!\n",
      "Avg loss: 0.003232487199968389!\n",
      "Avg loss: 0.003193465550226218!\n",
      "Avg loss: 0.0031712355189550093!\n",
      "Avg loss: 0.003159736766434068!\n",
      "Avg loss: 0.0031365238233992177!\n",
      "Avg loss: 0.0031616890859598084!\n",
      "Avg loss: 0.0031602880274613585!\n",
      "Avg loss: 0.0031747039692709114!\n",
      "Avg loss: 0.0031956662362562405!\n",
      "Avg loss: 0.003207703900002551!\n",
      "Avg loss: 0.0031972447682842293!\n",
      "Avg loss: 0.0031949306323606065!\n",
      "Avg loss: 0.003166458830832368!\n",
      "Avg loss: 0.003133542145735726!\n",
      "Avg loss: 0.003120746943524288!\n",
      "Avg loss: 0.00311867911240077!\n",
      "Avg loss: 0.0031035282507686406!\n",
      "Avg loss: 0.00310589589310667!\n",
      "Avg loss: 0.003100058950041617!\n",
      "Avg loss: 0.003090931526797019!\n",
      "Avg loss: 0.0030429534298306495!\n",
      "Avg loss: 0.003084138339914049!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:23:59,674]\u001B[0m Trial 15 finished with value: 0.0030452503569200003 and parameters: {'learning_rate': 0.0039353063847551726, 'batch_size': 325}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.0030452503569200003!\n",
      "Avg loss: 0.03588551804171908!\n",
      "Avg loss: 0.01867616284533954!\n",
      "Avg loss: 0.0157143963040467!\n",
      "Avg loss: 0.014519091541373281!\n",
      "Avg loss: 0.013016902498608747!\n",
      "Avg loss: 0.011395682033104879!\n",
      "Avg loss: 0.009806000386533797!\n",
      "Avg loss: 0.008795125684949725!\n",
      "Avg loss: 0.008160905417231454!\n",
      "Avg loss: 0.007722296403504038!\n",
      "Avg loss: 0.007398367708834206!\n",
      "Avg loss: 0.007257574693875879!\n",
      "Avg loss: 0.007109361944256259!\n",
      "Avg loss: 0.007002454716123102!\n",
      "Avg loss: 0.006917074054706157!\n",
      "Avg loss: 0.006860921030040498!\n",
      "Avg loss: 0.0068213300544298!\n",
      "Avg loss: 0.006737714926735591!\n",
      "Avg loss: 0.006738432715879857!\n",
      "Avg loss: 0.006728896447649186!\n",
      "Avg loss: 0.0067299850872817625!\n",
      "Avg loss: 0.006720115493327408!\n",
      "Avg loss: 0.006753515776456367!\n",
      "Avg loss: 0.006749097334964915!\n",
      "Avg loss: 0.006751873675088207!\n",
      "Avg loss: 0.004129467466130242!\n",
      "Avg loss: 0.0029289751667365043!\n",
      "Avg loss: 0.002850682899570076!\n",
      "Avg loss: 0.0028863561583863!\n",
      "Avg loss: 0.0028794309183110367!\n",
      "Avg loss: 0.0028857854737753363!\n",
      "Avg loss: 0.002802734763284016!\n",
      "Avg loss: 0.0028042910642898395!\n",
      "Avg loss: 0.0028041377494758186!\n",
      "Avg loss: 0.002803606767962273!\n",
      "Avg loss: 0.002798318735056809!\n",
      "Avg loss: 0.0027817976205655607!\n",
      "Avg loss: 0.0027907466412022634!\n",
      "Avg loss: 0.002788704164899856!\n",
      "Avg loss: 0.0027946102330354563!\n",
      "Avg loss: 0.0027869127582799322!\n",
      "Avg loss: 0.002796279011942654!\n",
      "Avg loss: 0.0027553373303996426!\n",
      "Avg loss: 0.0027578159252545573!\n",
      "Avg loss: 0.00275102346857095!\n",
      "Avg loss: 0.002724528686800177!\n",
      "Avg loss: 0.0027292087902048446!\n",
      "Avg loss: 0.0027110722242202363!\n",
      "Avg loss: 0.0026672468226027154!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:27:43,950]\u001B[0m Trial 16 finished with value: 0.0027487521199983807 and parameters: {'learning_rate': 0.003494515045859366, 'batch_size': 177}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.0027487521199983807!\n",
      "Avg loss: 0.0906903071908954!\n",
      "Avg loss: 0.05729616973977942!\n",
      "Avg loss: 0.0466085722359043!\n",
      "Avg loss: 0.042975384178119434!\n",
      "Avg loss: 0.04104779637779002!\n",
      "Avg loss: 0.03954956084599524!\n",
      "Avg loss: 0.03846268410064604!\n",
      "Avg loss: 0.037935125473748776!\n",
      "Avg loss: 0.03729202393652898!\n",
      "Avg loss: 0.03676896205307596!\n",
      "Avg loss: 0.036302125794913466!\n",
      "Avg loss: 0.03587470209892261!\n",
      "Avg loss: 0.035511136536584276!\n",
      "Avg loss: 0.035154625111608245!\n",
      "Avg loss: 0.034788562804636484!\n",
      "Avg loss: 0.0344028256730877!\n",
      "Avg loss: 0.034023691049030406!\n",
      "Avg loss: 0.033704307725428956!\n",
      "Avg loss: 0.033395162170315226!\n",
      "Avg loss: 0.03310145482690191!\n",
      "Avg loss: 0.032873655996457064!\n",
      "Avg loss: 0.032595831496327894!\n",
      "Avg loss: 0.032336717457722645!\n",
      "Avg loss: 0.03206906321766181!\n",
      "Avg loss: 0.03183853878820183!\n",
      "Avg loss: 0.03166350509527996!\n",
      "Avg loss: 0.03148561904517509!\n",
      "Avg loss: 0.031324339120891866!\n",
      "Avg loss: 0.031133792085619033!\n",
      "Avg loss: 0.03100217044270312!\n",
      "Avg loss: 0.030915440040542457!\n",
      "Avg loss: 0.030833660862784336!\n",
      "Avg loss: 0.030746130240704883!\n",
      "Avg loss: 0.030648416601493606!\n",
      "Avg loss: 0.030579646610101617!\n",
      "Avg loss: 0.030508391161928643!\n",
      "Avg loss: 0.03042997344404982!\n",
      "Avg loss: 0.030354267895959287!\n",
      "Avg loss: 0.030284373576940214!\n",
      "Avg loss: 0.03023933311357863!\n",
      "Avg loss: 0.03020738582643311!\n",
      "Avg loss: 0.030181680309531635!\n",
      "Avg loss: 0.03015285697553537!\n",
      "Avg loss: 0.030131306192932535!\n",
      "Avg loss: 0.030107667523887337!\n",
      "Avg loss: 0.03009050908483644!\n",
      "Avg loss: 0.030081218032453002!\n",
      "Avg loss: 0.030072024414983925!\n",
      "Avg loss: 0.030054832049276742!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:31:30,834]\u001B[0m Trial 17 finished with value: 0.03005052487817317 and parameters: {'learning_rate': 0.001695737394032725, 'batch_size': 285}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.03005052487817317!\n",
      "Avg loss: 0.17105342580759417!\n",
      "Avg loss: 0.08072108416162661!\n",
      "Avg loss: 0.06439574221134194!\n",
      "Avg loss: 0.05512263346325956!\n",
      "Avg loss: 0.04823990131118799!\n",
      "Avg loss: 0.04427770119417973!\n",
      "Avg loss: 0.04201753064455444!\n",
      "Avg loss: 0.040476103780351456!\n",
      "Avg loss: 0.039224424877437374!\n",
      "Avg loss: 0.03829493242934628!\n",
      "Avg loss: 0.03763683233842153!\n",
      "Avg loss: 0.03720756266657665!\n",
      "Avg loss: 0.036917604947346215!\n",
      "Avg loss: 0.03666353540133899!\n",
      "Avg loss: 0.03643276622354604!\n",
      "Avg loss: 0.036231782974867996!\n",
      "Avg loss: 0.03605940888992127!\n",
      "Avg loss: 0.035904379649648946!\n",
      "Avg loss: 0.03575928344911727!\n",
      "Avg loss: 0.03562232183216852!\n",
      "Avg loss: 0.03547446738165274!\n",
      "Avg loss: 0.035326171589906634!\n",
      "Avg loss: 0.035183527013293414!\n",
      "Avg loss: 0.03504605824071327!\n",
      "Avg loss: 0.03490371392068292!\n",
      "Avg loss: 0.03475831650902634!\n",
      "Avg loss: 0.034618248527216657!\n",
      "Avg loss: 0.0344807986373515!\n",
      "Avg loss: 0.03433142841814203!\n",
      "Avg loss: 0.034183499815490986!\n",
      "Avg loss: 0.03403405678192423!\n",
      "Avg loss: 0.03389339045524563!\n",
      "Avg loss: 0.033751826382662956!\n",
      "Avg loss: 0.03359666481374314!\n",
      "Avg loss: 0.0334377983234918!\n",
      "Avg loss: 0.03327027449443792!\n",
      "Avg loss: 0.03308950796622593!\n",
      "Avg loss: 0.03292504416496548!\n",
      "Avg loss: 0.032767868280684724!\n",
      "Avg loss: 0.032611054075720805!\n",
      "Avg loss: 0.0324536288996765!\n",
      "Avg loss: 0.03229353128121235!\n",
      "Avg loss: 0.032127136678593377!\n",
      "Avg loss: 0.03195642341238366!\n",
      "Avg loss: 0.03180093382736828!\n",
      "Avg loss: 0.031641553338365755!\n",
      "Avg loss: 0.03148849240790145!\n",
      "Avg loss: 0.03134612535832302!\n",
      "Avg loss: 0.031183907938841966!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:35:16,716]\u001B[0m Trial 18 finished with value: 0.031034934330606567 and parameters: {'learning_rate': 0.0006752944095269565, 'batch_size': 380}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.031034934330606567!\n",
      "Avg loss: 0.024264213624993574!\n",
      "Avg loss: 0.0178300689530478!\n",
      "Avg loss: 0.014299964476092206!\n",
      "Avg loss: 0.010626720729043542!\n",
      "Avg loss: 0.008316163746676049!\n",
      "Avg loss: 0.007576746495813564!\n",
      "Avg loss: 0.007588987048503227!\n",
      "Avg loss: 0.0074319713127463894!\n",
      "Avg loss: 0.007412689730501806!\n",
      "Avg loss: 0.007293106767804527!\n",
      "Avg loss: 0.007174364769063206!\n",
      "Avg loss: 0.007046687548181694!\n",
      "Avg loss: 0.007093691061044614!\n",
      "Avg loss: 0.007093080663179759!\n",
      "Avg loss: 0.007082191674209499!\n",
      "Avg loss: 0.007100624861824291!\n",
      "Avg loss: 0.0070537488053736!\n",
      "Avg loss: 0.007079660891782003!\n",
      "Avg loss: 0.00707825699069346!\n",
      "Avg loss: 0.007075946532718352!\n",
      "Avg loss: 0.006992039992589376!\n",
      "Avg loss: 0.007115052616677228!\n",
      "Avg loss: 0.007017368665187436!\n",
      "Avg loss: 0.0070988804333729175!\n",
      "Avg loss: 0.007230815400373149!\n",
      "Avg loss: 0.007262415891968065!\n",
      "Avg loss: 0.007379836057664841!\n",
      "Avg loss: 0.007232721163550745!\n",
      "Avg loss: 0.00700439080963422!\n",
      "Avg loss: 0.007048598006998277!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:37:37,765]\u001B[0m Trial 19 finished with value: 0.006998994174402464 and parameters: {'learning_rate': 0.008602818229264965, 'batch_size': 95}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.006998994174402464!\n",
      "Avg loss: 0.06439790616756082!\n",
      "Avg loss: 0.02981791472093!\n",
      "Avg loss: 0.024374530131628075!\n",
      "Avg loss: 0.022802794246880954!\n",
      "Avg loss: 0.020902864756149338!\n",
      "Avg loss: 0.01945396745695488!\n",
      "Avg loss: 0.018062296397226354!\n",
      "Avg loss: 0.017005586597492392!\n",
      "Avg loss: 0.016163892365546786!\n",
      "Avg loss: 0.015345721791281387!\n",
      "Avg loss: 0.014832960133451444!\n",
      "Avg loss: 0.014694904482936037!\n",
      "Avg loss: 0.014644298213750944!\n",
      "Avg loss: 0.014534976393914852!\n",
      "Avg loss: 0.014375635436626583!\n",
      "Avg loss: 0.014189552497058718!\n",
      "Avg loss: 0.014107993339954083!\n",
      "Avg loss: 0.014101693335924585!\n",
      "Avg loss: 0.014232095699248555!\n",
      "Avg loss: 0.01420967540995795!\n",
      "Avg loss: 0.01408996904110886!\n",
      "Avg loss: 0.014032385013758496!\n",
      "Avg loss: 0.013989420299636932!\n",
      "Avg loss: 0.013945071116898756!\n",
      "Avg loss: 0.01391937325181234!\n",
      "Avg loss: 0.013924328499248984!\n",
      "Avg loss: 0.01389892309170006!\n",
      "Avg loss: 0.014428381885282376!\n",
      "Avg loss: 0.013942585907577841!\n",
      "Avg loss: 0.013925965195216482!\n",
      "Avg loss: 0.013917700577774944!\n",
      "Avg loss: 0.013914954975704741!\n",
      "Avg loss: 0.013955950308979186!\n",
      "Avg loss: 0.014519990206001577!\n",
      "Avg loss: 0.014240811599903903!\n",
      "Avg loss: 0.013969675421815403!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:40:22,910]\u001B[0m Trial 20 finished with value: 0.013911983464659666 and parameters: {'learning_rate': 0.004791558530220652, 'batch_size': 203}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.013911983464659666!\n",
      "Avg loss: 0.02516800160961882!\n",
      "Avg loss: 0.0162806665075993!\n",
      "Avg loss: 0.0142142421570425!\n",
      "Avg loss: 0.012592575305157394!\n",
      "Avg loss: 0.011366350300849662!\n",
      "Avg loss: 0.010701684178339959!\n",
      "Avg loss: 0.00943125515250558!\n",
      "Avg loss: 0.007584143326687802!\n",
      "Avg loss: 0.005590357679443121!\n",
      "Avg loss: 0.004436049749384891!\n",
      "Avg loss: 0.00385288048122015!\n",
      "Avg loss: 0.003539529123770238!\n",
      "Avg loss: 0.0033354891620255797!\n",
      "Avg loss: 0.0032587483201752176!\n",
      "Avg loss: 0.0032755252866798305!\n",
      "Avg loss: 0.0032556141288185084!\n",
      "Avg loss: 0.0032676842524798043!\n",
      "Avg loss: 0.0032479618086841138!\n",
      "Avg loss: 0.003246379251397684!\n",
      "Avg loss: 0.003239799163656162!\n",
      "Avg loss: 0.0032291379660444024!\n",
      "Avg loss: 0.0031937881877240866!\n",
      "Avg loss: 0.0031754442506151167!\n",
      "Avg loss: 0.003131697773901231!\n",
      "Avg loss: 0.0031297628469549466!\n",
      "Avg loss: 0.0030803400217201352!\n",
      "Avg loss: 0.003048998273747517!\n",
      "Avg loss: 0.0030196112130311883!\n",
      "Avg loss: 0.003022992567604504!\n",
      "Avg loss: 0.002992753388101294!\n",
      "Avg loss: 0.0030092936048853975!\n",
      "Avg loss: 0.0029885061668645696!\n",
      "Avg loss: 0.0029531823140976445!\n",
      "Avg loss: 0.0030806449110170313!\n",
      "Avg loss: 0.003039801936332358!\n",
      "Avg loss: 0.0030410091838424243!\n",
      "Avg loss: 0.0030038233248977334!\n",
      "Avg loss: 0.003025897591462182!\n",
      "Avg loss: 0.0030727275022985933!\n",
      "Avg loss: 0.0031057316846367714!\n",
      "Avg loss: 0.0030984666860003054!\n",
      "Avg loss: 0.003122750421671537!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:43:36,359]\u001B[0m Trial 21 finished with value: 0.003034450574238436 and parameters: {'learning_rate': 0.0037088342720976277, 'batch_size': 169}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.003034450574238436!\n",
      "Avg loss: 0.048791118246324414!\n",
      "Avg loss: 0.046698719367874834!\n",
      "Avg loss: 0.04523473596823946!\n",
      "Avg loss: 0.041825398638207716!\n",
      "Avg loss: 0.03954553177357772!\n",
      "Avg loss: 0.038582289832294864!\n",
      "Avg loss: 0.038245763445814776!\n",
      "Avg loss: 0.03810244375966242!\n",
      "Avg loss: 0.03806260741487647!\n",
      "Avg loss: 0.03805627835596575!\n",
      "Avg loss: 0.038047366585444664!\n",
      "Avg loss: 0.038029621546583334!\n",
      "Avg loss: 0.038038583773765976!\n",
      "Avg loss: 0.03802631296383417!\n",
      "Avg loss: 0.038034168014120094!\n",
      "Avg loss: 0.03804969447624428!\n",
      "Avg loss: 0.03798927762034742!\n",
      "Avg loss: 0.0380184087974275!\n",
      "Avg loss: 0.03802144946758915!\n",
      "Avg loss: 0.03799273965584415!\n",
      "Avg loss: 0.03799018401784547!\n",
      "Avg loss: 0.03800360536839212!\n",
      "Avg loss: 0.037997380681009135!\n",
      "Avg loss: 0.038002690549382795!\n",
      "Avg loss: 0.03799250117831525!\n",
      "Avg loss: 0.03798873573687938!\n",
      "Avg loss: 0.03820239076758514!\n",
      "Avg loss: 0.03812582691790008!\n",
      "Avg loss: 0.03835079264299066!\n",
      "Avg loss: 0.0383119064743786!\n",
      "Avg loss: 0.03814587693520127!\n",
      "Avg loss: 0.03810728564539191!\n",
      "Avg loss: 0.038067729650639856!\n",
      "Avg loss: 0.038011168320163845!\n",
      "Avg loss: 0.03800674020207881!\n",
      "Avg loss: 0.03796387908762495!\n",
      "Avg loss: 0.03800596117499863!\n",
      "Avg loss: 0.038009660680215086!\n",
      "Avg loss: 0.038076541436115344!\n",
      "Avg loss: 0.0380699986903317!\n",
      "Avg loss: 0.03811262054434245!\n",
      "Avg loss: 0.03829627798971471!\n",
      "Avg loss: 0.038540151534236636!\n",
      "Avg loss: 0.03798151289596139!\n",
      "Avg loss: 0.038108974704136295!\n",
      "Avg loss: 0.015049217956130723!\n",
      "Avg loss: 0.014743427526986246!\n",
      "Avg loss: 0.014752370314809842!\n",
      "Avg loss: 0.014756477237285959!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:47:25,037]\u001B[0m Trial 22 finished with value: 0.014747809650269285 and parameters: {'learning_rate': 0.0034053098008769494, 'batch_size': 71}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.014747809650269285!\n",
      "Avg loss: 0.09138054800469264!\n",
      "Avg loss: 0.04927027384262713!\n",
      "Avg loss: 0.04236370288348864!\n",
      "Avg loss: 0.03975486444609102!\n",
      "Avg loss: 0.03755834778805701!\n",
      "Avg loss: 0.036666108047787034!\n",
      "Avg loss: 0.015308020122756878!\n",
      "Avg loss: 0.0119450074134789!\n",
      "Avg loss: 0.011040142501865337!\n",
      "Avg loss: 0.010377536018918054!\n",
      "Avg loss: 0.009818033773303878!\n",
      "Avg loss: 0.009207078416967822!\n",
      "Avg loss: 0.008559598690730443!\n",
      "Avg loss: 0.007830931418274772!\n",
      "Avg loss: 0.007110793076685684!\n",
      "Avg loss: 0.006473413265784949!\n",
      "Avg loss: 0.005905906075118249!\n",
      "Avg loss: 0.0053244120621003515!\n",
      "Avg loss: 0.004858327922227957!\n",
      "Avg loss: 0.004451323629131346!\n",
      "Avg loss: 0.004144503486465401!\n",
      "Avg loss: 0.003915614898013088!\n",
      "Avg loss: 0.0037733340556188517!\n",
      "Avg loss: 0.0036824811805818754!\n",
      "Avg loss: 0.003613475976203499!\n",
      "Avg loss: 0.003562612168549346!\n",
      "Avg loss: 0.0035318751379110787!\n",
      "Avg loss: 0.0034961460722933695!\n",
      "Avg loss: 0.0034949203031582722!\n",
      "Avg loss: 0.003497369618868446!\n",
      "Avg loss: 0.003457573846635339!\n",
      "Avg loss: 0.00345802126783198!\n",
      "Avg loss: 0.0034453777016572916!\n",
      "Avg loss: 0.003416968127874037!\n",
      "Avg loss: 0.003404594917189265!\n",
      "Avg loss: 0.003378917481478824!\n",
      "Avg loss: 0.0033575946904003664!\n",
      "Avg loss: 0.0033452075929199785!\n",
      "Avg loss: 0.0033266316032429855!\n",
      "Avg loss: 0.0033021277146587738!\n",
      "Avg loss: 0.003284089113749369!\n",
      "Avg loss: 0.0032683555572290373!\n",
      "Avg loss: 0.003229185151085841!\n",
      "Avg loss: 0.00321379445602177!\n",
      "Avg loss: 0.0031905850533830125!\n",
      "Avg loss: 0.003174834934635544!\n",
      "Avg loss: 0.003150292257005353!\n",
      "Avg loss: 0.0031402562450584576!\n",
      "Avg loss: 0.0031261786034754422!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:51:09,649]\u001B[0m Trial 23 finished with value: 0.003122649617535269 and parameters: {'learning_rate': 0.0020963572754360057, 'batch_size': 293}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.003122649617535269!\n",
      "Avg loss: 0.023844344859442604!\n",
      "Avg loss: 0.016297138603475994!\n",
      "Avg loss: 0.0147270548356571!\n",
      "Avg loss: 0.012252495449402124!\n",
      "Avg loss: 0.010547114527793255!\n",
      "Avg loss: 0.008446837905674702!\n",
      "Avg loss: 0.007568539613332175!\n",
      "Avg loss: 0.0073423764229008474!\n",
      "Avg loss: 0.007181571801363242!\n",
      "Avg loss: 0.007057951287406559!\n",
      "Avg loss: 0.006985549036559728!\n",
      "Avg loss: 0.006970910674858097!\n",
      "Avg loss: 0.00693023826069239!\n",
      "Avg loss: 0.0069437244641042655!\n",
      "Avg loss: 0.0069251126062300624!\n",
      "Avg loss: 0.007006904116930233!\n",
      "Avg loss: 0.007012508582141313!\n",
      "Avg loss: 0.0069086055084247935!\n",
      "Avg loss: 0.007136177696561776!\n",
      "Avg loss: 0.007044153387496174!\n",
      "Avg loss: 0.0071102760417299056!\n",
      "Avg loss: 0.0070241416029822815!\n",
      "Avg loss: 0.007184813836471531!\n",
      "Avg loss: 0.006936035818510265!\n",
      "Avg loss: 0.007130717875699195!\n",
      "Avg loss: 0.007421033196551188!\n",
      "Avg loss: 0.007259107845476457!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:53:14,296]\u001B[0m Trial 24 finished with value: 0.007185523970156139 and parameters: {'learning_rate': 0.005743823437696367, 'batch_size': 142}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.007185523970156139!\n",
      "Avg loss: 0.03044191912308549!\n",
      "Avg loss: 0.01663098837342841!\n",
      "Avg loss: 0.012892939321165387!\n",
      "Avg loss: 0.011350898856943467!\n",
      "Avg loss: 0.00952616743682575!\n",
      "Avg loss: 0.008247165413264574!\n",
      "Avg loss: 0.007469295510527781!\n",
      "Avg loss: 0.006356526925496589!\n",
      "Avg loss: 0.005712691565857737!\n",
      "Avg loss: 0.0050938064966444705!\n",
      "Avg loss: 0.004676194498762765!\n",
      "Avg loss: 0.004448697691963906!\n",
      "Avg loss: 0.004275186730625117!\n",
      "Avg loss: 0.004163050247726604!\n",
      "Avg loss: 0.004034622282738335!\n",
      "Avg loss: 0.003940718604735443!\n",
      "Avg loss: 0.003841941917164486!\n",
      "Avg loss: 0.0037532362217910006!\n",
      "Avg loss: 0.003682634808883292!\n",
      "Avg loss: 0.0036195853116138157!\n",
      "Avg loss: 0.0035727361850052177!\n",
      "Avg loss: 0.00352355336703117!\n",
      "Avg loss: 0.0034620628148468342!\n",
      "Avg loss: 0.003431395114387369!\n",
      "Avg loss: 0.0034150677358845663!\n",
      "Avg loss: 0.003393834781206899!\n",
      "Avg loss: 0.0033722175737627876!\n",
      "Avg loss: 0.0033641834959851806!\n",
      "Avg loss: 0.003352384641140374!\n",
      "Avg loss: 0.0033412901213908295!\n",
      "Avg loss: 0.0033351108337446806!\n",
      "Avg loss: 0.0033099929281326236!\n",
      "Avg loss: 0.003300904631318796!\n",
      "Avg loss: 0.0032900976647058!\n",
      "Avg loss: 0.0032992336623998628!\n",
      "Avg loss: 0.0032812272799334448!\n",
      "Avg loss: 0.0032946123516999384!\n",
      "Avg loss: 0.0032700758056290573!\n",
      "Avg loss: 0.003275373354431472!\n",
      "Avg loss: 0.0032718326413990442!\n",
      "Avg loss: 0.00327877618093751!\n",
      "Avg loss: 0.003284192690092853!\n",
      "Avg loss: 0.003274918703255334!\n",
      "Avg loss: 0.0033023649782565044!\n",
      "Avg loss: 0.0032979805714582256!\n",
      "Avg loss: 0.0032860883068103228!\n",
      "Avg loss: 0.0033012999872439716!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-05 23:56:49,326]\u001B[0m Trial 25 finished with value: 0.0033266127237981118 and parameters: {'learning_rate': 0.0031882353776622324, 'batch_size': 187}. Best is trial 7 with value: 0.0022408253588251806.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.0033266127237981118!\n",
      "Avg loss: 0.30669668210820134!\n",
      "Avg loss: 0.03019661248139526!\n",
      "Avg loss: 0.019258338341772697!\n",
      "Avg loss: 0.01476801323402952!\n",
      "Avg loss: 0.012162256934783865!\n",
      "Avg loss: 0.010894077592350124!\n",
      "Avg loss: 0.010028109798474846!\n",
      "Avg loss: 0.009221009059653538!\n",
      "Avg loss: 0.008583694839615906!\n",
      "Avg loss: 0.008022315506695616!\n",
      "Avg loss: 0.007530471136846841!\n",
      "Avg loss: 0.006964570085562912!\n",
      "Avg loss: 0.006307127022598986!\n",
      "Avg loss: 0.005652248262251542!\n",
      "Avg loss: 0.005232984908390556!\n",
      "Avg loss: 0.004804337355057956!\n",
      "Avg loss: 0.004442183944660697!\n",
      "Avg loss: 0.0040953454419598466!\n",
      "Avg loss: 0.003803581081990359!\n",
      "Avg loss: 0.0035619031001796007!\n",
      "Avg loss: 0.003358584150060638!\n",
      "Avg loss: 0.0031801257495774984!\n",
      "Avg loss: 0.003028098103212947!\n",
      "Avg loss: 0.0029122345839176626!\n",
      "Avg loss: 0.002820616447448288!\n",
      "Avg loss: 0.002731694829568986!\n",
      "Avg loss: 0.0026667068182467833!\n",
      "Avg loss: 0.0026206942631593305!\n",
      "Avg loss: 0.002574891087297969!\n",
      "Avg loss: 0.0025315027786848776!\n",
      "Avg loss: 0.0025060525439175763!\n",
      "Avg loss: 0.0024844566648987722!\n",
      "Avg loss: 0.002462863168901568!\n",
      "Avg loss: 0.002442683383762868!\n",
      "Avg loss: 0.0024313996735618464!\n",
      "Avg loss: 0.002414162202150565!\n",
      "Avg loss: 0.0023979413984591092!\n",
      "Avg loss: 0.0023846821041394258!\n",
      "Avg loss: 0.002366471224494475!\n",
      "Avg loss: 0.0023310521165700974!\n",
      "Avg loss: 0.0023120537109827467!\n",
      "Avg loss: 0.00229295891240864!\n",
      "Avg loss: 0.0022757448768101307!\n",
      "Avg loss: 0.0022537882290210973!\n",
      "Avg loss: 0.0022335071502598987!\n",
      "Avg loss: 0.0022101624428713627!\n",
      "Avg loss: 0.0021824729610192325!\n",
      "Avg loss: 0.0021601477854608668!\n",
      "Avg loss: 0.0021403852075838766!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-06 00:00:30,411]\u001B[0m Trial 26 finished with value: 0.0021155083512607725 and parameters: {'learning_rate': 0.0014335761400056032, 'batch_size': 234}. Best is trial 26 with value: 0.0021155083512607725.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.0021155083512607725!\n",
      "Avg loss: 0.09367866500875295!\n",
      "Avg loss: 0.058500812665756664!\n",
      "Avg loss: 0.04309998249205656!\n",
      "Avg loss: 0.036132964570788614!\n",
      "Avg loss: 0.03234303018833196!\n",
      "Avg loss: 0.030376858557437186!\n",
      "Avg loss: 0.0287350122331054!\n",
      "Avg loss: 0.027530366268704!\n",
      "Avg loss: 0.026692796300833235!\n",
      "Avg loss: 0.026118214564474843!\n",
      "Avg loss: 0.025699431210508966!\n",
      "Avg loss: 0.02533645476928938!\n",
      "Avg loss: 0.025004611725625906!\n",
      "Avg loss: 0.024686648350183287!\n",
      "Avg loss: 0.024381669048782435!\n",
      "Avg loss: 0.02409525994752138!\n",
      "Avg loss: 0.02381451882160479!\n",
      "Avg loss: 0.02353063714730935!\n",
      "Avg loss: 0.02322939721692854!\n",
      "Avg loss: 0.022925288077210167!\n",
      "Avg loss: 0.022636187427112504!\n",
      "Avg loss: 0.02236251457261387!\n",
      "Avg loss: 0.02209802322386191!\n",
      "Avg loss: 0.02184048493213977!\n",
      "Avg loss: 0.02157374672382183!\n",
      "Avg loss: 0.0213025584812846!\n",
      "Avg loss: 0.02104678209473547!\n",
      "Avg loss: 0.020790263461550017!\n",
      "Avg loss: 0.02053329732109192!\n",
      "Avg loss: 0.020277388001149952!\n",
      "Avg loss: 0.020019999952510432!\n",
      "Avg loss: 0.019783025250967277!\n",
      "Avg loss: 0.019554061650777516!\n",
      "Avg loss: 0.019324595144782786!\n",
      "Avg loss: 0.019100804316461897!\n",
      "Avg loss: 0.01887350651724885!\n",
      "Avg loss: 0.01864905157517394!\n",
      "Avg loss: 0.01840788940532393!\n",
      "Avg loss: 0.01816820490592562!\n",
      "Avg loss: 0.01792969852410773!\n",
      "Avg loss: 0.017692608698540162!\n",
      "Avg loss: 0.017449241116271226!\n",
      "Avg loss: 0.01722507796491987!\n",
      "Avg loss: 0.017011284332843636!\n",
      "Avg loss: 0.016813055213823855!\n",
      "Avg loss: 0.016633372432521727!\n",
      "Avg loss: 0.016462055808708426!\n",
      "Avg loss: 0.016294314825335953!\n",
      "Avg loss: 0.016138555256752653!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-06 00:04:15,315]\u001B[0m Trial 27 finished with value: 0.01599919926177853 and parameters: {'learning_rate': 0.0012616292851266944, 'batch_size': 411}. Best is trial 26 with value: 0.0021155083512607725.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.01599919926177853!\n",
      "Avg loss: 0.27033690814170136!\n",
      "Avg loss: 0.10091371328279886!\n",
      "Avg loss: 0.05677331822027906!\n",
      "Avg loss: 0.040963673626541776!\n",
      "Avg loss: 0.03319759888790627!\n",
      "Avg loss: 0.027849260474628428!\n",
      "Avg loss: 0.024000545820555868!\n",
      "Avg loss: 0.021384820407579823!\n",
      "Avg loss: 0.01961595552146283!\n",
      "Avg loss: 0.018439215662592507!\n",
      "Avg loss: 0.01746668318765138!\n",
      "Avg loss: 0.016622753433061212!\n",
      "Avg loss: 0.015811605439756345!\n",
      "Avg loss: 0.015095696866877287!\n",
      "Avg loss: 0.01444765867782422!\n",
      "Avg loss: 0.013882694551769426!\n",
      "Avg loss: 0.013388726240620468!\n",
      "Avg loss: 0.012969061754504732!\n",
      "Avg loss: 0.012610424048341264!\n",
      "Avg loss: 0.012304961671286547!\n",
      "Avg loss: 0.012017202328687747!\n",
      "Avg loss: 0.011784711401918781!\n",
      "Avg loss: 0.011564569354430428!\n",
      "Avg loss: 0.011367167771361484!\n",
      "Avg loss: 0.011185011520003178!\n",
      "Avg loss: 0.011012358096846194!\n",
      "Avg loss: 0.010842465469926319!\n",
      "Avg loss: 0.010680536540027786!\n",
      "Avg loss: 0.010522394208782914!\n",
      "Avg loss: 0.010368583879256365!\n",
      "Avg loss: 0.010217368967015987!\n",
      "Avg loss: 0.010066903284137849!\n",
      "Avg loss: 0.009913743371782257!\n",
      "Avg loss: 0.009760795069058288!\n",
      "Avg loss: 0.009612386589511149!\n",
      "Avg loss: 0.009463916508499471!\n",
      "Avg loss: 0.009311868089485314!\n",
      "Avg loss: 0.009164242272662783!\n",
      "Avg loss: 0.009029755743785355!\n",
      "Avg loss: 0.008881620491668062!\n",
      "Avg loss: 0.008734623862587311!\n",
      "Avg loss: 0.008584902491381947!\n",
      "Avg loss: 0.00843940557248421!\n",
      "Avg loss: 0.008286504239920101!\n",
      "Avg loss: 0.008134761406602124!\n",
      "Avg loss: 0.007987961305087613!\n",
      "Avg loss: 0.007846030060043634!\n",
      "Avg loss: 0.0077080993192109384!\n",
      "Avg loss: 0.007573585369250872!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-06 00:07:59,345]\u001B[0m Trial 28 finished with value: 0.00743870951443024 and parameters: {'learning_rate': 0.0001818627374310395, 'batch_size': 230}. Best is trial 26 with value: 0.0021155083512607725.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.00743870951443024!\n",
      "Avg loss: 0.06916191261658573!\n",
      "Avg loss: 0.031631831810985514!\n",
      "Avg loss: 0.020423128052126552!\n",
      "Avg loss: 0.015737655833550152!\n",
      "Avg loss: 0.0136270623361551!\n",
      "Avg loss: 0.01241118761193831!\n",
      "Avg loss: 0.011626235461755806!\n",
      "Avg loss: 0.010992061979688196!\n",
      "Avg loss: 0.010481765114323263!\n",
      "Avg loss: 0.009986623300633667!\n",
      "Avg loss: 0.009414184249933216!\n",
      "Avg loss: 0.008870418954555271!\n",
      "Avg loss: 0.008370568492704735!\n",
      "Avg loss: 0.007897625193025398!\n",
      "Avg loss: 0.00745523467677172!\n",
      "Avg loss: 0.007037219048979798!\n",
      "Avg loss: 0.006663461652430917!\n",
      "Avg loss: 0.0063234191710251535!\n",
      "Avg loss: 0.006015029969253038!\n",
      "Avg loss: 0.005685056961798153!\n",
      "Avg loss: 0.005331691574356125!\n",
      "Avg loss: 0.004958253249913691!\n",
      "Avg loss: 0.004552947279927011!\n",
      "Avg loss: 0.004205914519100431!\n",
      "Avg loss: 0.003918418592220735!\n",
      "Avg loss: 0.003671807241371315!\n",
      "Avg loss: 0.0034862889136706324!\n",
      "Avg loss: 0.0033394647230804674!\n",
      "Avg loss: 0.0032170173082458945!\n",
      "Avg loss: 0.003116301821541391!\n",
      "Avg loss: 0.0030223118571749423!\n",
      "Avg loss: 0.0029328904744253697!\n",
      "Avg loss: 0.002853756109673253!\n",
      "Avg loss: 0.002790247085979796!\n",
      "Avg loss: 0.002740472357138769!\n",
      "Avg loss: 0.0026920237243324607!\n",
      "Avg loss: 0.0026504751584199597!\n",
      "Avg loss: 0.002607692249238976!\n",
      "Avg loss: 0.002564744802541245!\n",
      "Avg loss: 0.0025286179235378035!\n",
      "Avg loss: 0.002495632778080743!\n",
      "Avg loss: 0.0024664169155209!\n",
      "Avg loss: 0.0024362967658504973!\n",
      "Avg loss: 0.0024108570068097634!\n",
      "Avg loss: 0.002391079124350404!\n",
      "Avg loss: 0.0023787779781526807!\n",
      "Avg loss: 0.0023687114394389482!\n",
      "Avg loss: 0.0023584513609464296!\n",
      "Avg loss: 0.002351713943952981!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2023-01-06 00:11:45,568]\u001B[0m Trial 29 finished with value: 0.002342224313729295 and parameters: {'learning_rate': 0.001075924155665366, 'batch_size': 262}. Best is trial 26 with value: 0.0021155083512607725.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.002342224313729295!\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "{'learning_rate': 0.0014335761400056032, 'batch_size': 234}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe()\n",
    "study.best_trial.params"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
